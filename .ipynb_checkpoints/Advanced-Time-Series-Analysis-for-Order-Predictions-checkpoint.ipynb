{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d015c10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20202f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required Pandas library.\n",
    "\n",
    "# Import the Pandas library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be49d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data from 'orders.csv' into a DataFrame, setting 'date' as the index and parsing it appropriately.\n",
    "\n",
    "# Load data from 'orders.csv' with 'date' as index and parsed as date\n",
    "df = pd.read_csv(\"orders.csv\", parse_dates=True, index_col = ['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47846118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the last five records to ensure correct data loading.\n",
    "\n",
    "# Preview the last five entries in the DataFrame\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028cc20e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain and display a statistical summary of the DataFrame to inspect its data distribution.\n",
    "\n",
    "# Retrieve and print a statistical summary of the DataFrame\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04b735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ef2e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by importing the necessary Matplotlib library.\n",
    "\n",
    "# Import the required Matplotlib library for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad07475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a figure to host the three subplots with a suitable size for clear visualization. \n",
    "# Create the first subplot to visualize 'orders' over time.\n",
    "\n",
    "# Initialize a figure for the subplots\n",
    "plt.figure(figsize=(15, 6))\n",
    " \n",
    "# Create a subplot to visualize 'orders' over time\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(df.index, df['orders'])\n",
    "plt.title('Orders Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Orders')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f0bd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the 'marketing_investment' against time in the second subplot. Adjust the layout to ensure no overlaps and display the plotted charts.\n",
    "\n",
    "# Initialize a figure for the subplots\n",
    "plt.figure(figsize=(15, 6))\n",
    " \n",
    "# Create a subplot to visualize 'orders' over time\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(df.index, df['orders'])\n",
    "plt.title('Orders Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Orders')\n",
    " \n",
    "# Add a subplot to visualize 'marketing_investment' over time\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(df.index, df['marketing_investment'])\n",
    "plt.title('Marketing Investment Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Marketing Investment')\n",
    " \n",
    "# Adjust the layout and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd599778",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, visualize the 'discount_rate' over time in the third subplot.\n",
    "\n",
    "# Initialize a figure for the subplots\n",
    "plt.figure(figsize=(15, 6))\n",
    " \n",
    "# Create a subplot to visualize 'orders' over time\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(df.index, df['orders'])\n",
    "plt.title('Orders Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Orders')\n",
    " \n",
    "# Add a subplot to visualize 'marketing_investment' over time\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.plot(df.index, df['marketing_investment'])\n",
    "plt.title('Marketing Investment Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Marketing Investment')\n",
    " \n",
    "# Add a subplot to visualize 'discount_rate' over time\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.plot(df.index, df['discount_rate'])\n",
    "plt.title('Discount Rate Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Discount Rate')\n",
    " \n",
    "# Adjust the layout and display the figure\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f0961a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dae5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required library for STL decomposition.\n",
    "\n",
    "# Required Libraries\n",
    "from statsmodels.tsa.seasonal import STL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a481fad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decompose the 'orders' time series into its trend, seasonal, and residual components. Assuming daily data, set the seasonality period to 365 days.\n",
    "\n",
    "# Decompose the 'orders' time series using STL\n",
    "stl = STL(df['orders'], period=365)  # Assuming daily data and 365 days for seasonality\n",
    "result = stl.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca46832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the respective components from the STL result.\n",
    "\n",
    "# Extract the trend, seasonal, and residual components\n",
    "trend = result.trend\n",
    "seasonal = result.seasonal\n",
    "residual = result.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c480ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot and visualize the original time series data alongside the decomposed components.\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(15, 8))\n",
    " \n",
    "# Original Data\n",
    "plt.subplot(4, 1, 1)\n",
    "plt.plot(df.index, df['orders'], label='Original')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Original Time Series')\n",
    " \n",
    "# Trend Component\n",
    "plt.subplot(4, 1, 2)\n",
    "plt.plot(df.index, trend, label='Trend')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Trend Component')\n",
    " \n",
    "# Seasonal Component\n",
    "plt.subplot(4, 1, 3)\n",
    "plt.plot(df.index, seasonal, label='Seasonal')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Seasonal Component')\n",
    " \n",
    "# Residual Component\n",
    "plt.subplot(4, 1, 4)\n",
    "plt.plot(df.index, residual, label='Residual')\n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Residual Component')\n",
    " \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b655caf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d3376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the trend and seasonal components to the DataFrame.\n",
    "\n",
    "# Add trend and seasonal components to the dataframe\n",
    "df['trend'] = result.trend\n",
    "df['seasonal'] = result.seasonal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556cc706",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the DataFrame by eliminating any rows that might have NaN values, ensuring a seamless dataset.\n",
    "\n",
    "# We'll need to remove any remaining NaN values after decomposition\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafce233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the first 5 rows to check the outcome.\n",
    "\n",
    "# Preview the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340115e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b5b90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary libraries and tools for data processing and LSTM modeling.\n",
    "\n",
    "# Required Libraries\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638b2185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilize the MinMaxScaler to scale the dataset for neural network modeling.\n",
    "\n",
    "# Preprocess data for LSTM\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a774cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment the data into feature set (X) and target variable (y).\n",
    "\n",
    "# Isolate X and Y\n",
    "X = scaled_data[:, 1:]\n",
    "y = scaled_data[:, 0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9277a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alter the shape of the feature set to be compatible with LSTM layers.\n",
    "\n",
    "# Reshape X for LSTM\n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a12bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing sets, ensuring you separate the last 31 records for testing.\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test = X[:-31], X[-31:]\n",
    "y_train, y_test = y[:-31], y[-31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a0e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Design and compile an LSTM model architecture, specifying the layers, activation functions, and loss optimization strategy.\n",
    "\n",
    "# LSTM Model architecture\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, \n",
    "               activation='relu', \n",
    "               input_shape=(X_train.shape[1], \n",
    "                            X_train.shape[2])))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a8ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the prepared LSTM model using the segmented training data.\n",
    "\n",
    "# Train the LSTM model\n",
    "model.fit(X_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06e45d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c04eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required metric utilities:\n",
    "\n",
    "# Importing the metric utilities\n",
    "from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fff3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test data using LSTM model:\n",
    "\n",
    "# Predicting the orders for the last 31 days\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e481b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert the scaling process for predicted data:\n",
    "\n",
    "# Setting up a zero matrix to facilitate the reverse scaling process\n",
    "num_features = df.shape[1]\n",
    "zero_matrix = np.zeros((y_pred.shape[0], num_features - 1))\n",
    " \n",
    "# Merging the predictions with the zero matrix\n",
    "inverse_input = np.hstack((y_pred, zero_matrix))\n",
    " \n",
    "# Reverting the scaling to obtain predicted values in the original order magnitude\n",
    "predicted_orders = scaler.inverse_transform(inverse_input)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e25f798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the actual last 31 days of orders for comparison:\n",
    "\n",
    "# Retrieving the actual orders for the last 31 days\n",
    "actual_orders = df['orders'].values[-31:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2d3856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the performance metrics:\n",
    "\n",
    "# Calculating Mean Absolute Error\n",
    "MAE = mean_absolute_error(actual_orders, predicted_orders)\n",
    " \n",
    "# Calculating Mean Absolute Percentage Error\n",
    "MAPE = mean_absolute_percentage_error(actual_orders, predicted_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8fb929b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the derived esults:\n",
    "\n",
    "# Printing the derived performance metrics\n",
    "print(f\"Mean Absolute Error (MAE): {MAE}\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {MAPE}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76758a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary layer:\n",
    "\n",
    "# Importing the Dropout layer from Keras\n",
    "from tensorflow.keras.layers import Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c7cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the advanced LSTM model:\n",
    "\n",
    "# Setting up an advanced LSTM architecture\n",
    "advanced_model = Sequential()\n",
    " \n",
    "# First LSTM layer along with Dropout\n",
    "advanced_model.add(LSTM(100, \n",
    "                        activation='relu', \n",
    "                        return_sequences=True,\n",
    "                        input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "advanced_model.add(Dropout(0.2))\n",
    " \n",
    "# Second LSTM layer followed by Dropout\n",
    "advanced_model.add(LSTM(50, activation='relu'))\n",
    "advanced_model.add(Dropout(0.2))\n",
    " \n",
    "# Final Dense layer\n",
    "advanced_model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d43453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train the model:\n",
    "\n",
    "# Compiling the model\n",
    "advanced_model.compile(optimizer='adam', loss='mae')\n",
    " \n",
    "# Model training\n",
    "advanced_model.fit(X_train, y_train, epochs=50, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a7359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with the advanced model:\n",
    "\n",
    "# Making predictions with the advanced model\n",
    "y_pred_advanced = advanced_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a1b7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revert scaling for predicted data:\n",
    "\n",
    "# Constructing an inverse input matrix with predictions and zero columns\n",
    "inverse_input_advanced = np.hstack((y_pred_advanced, zero_matrix))\n",
    " \n",
    "# Undoing the scaling to get predicted values in the original order magnitude\n",
    "predicted_orders_advanced = scaler.inverse_transform(inverse_input_advanced)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bf2749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the performance metrics for the advanced model:\n",
    "\n",
    "# Calculating the Mean Absolute Error for the advanced model\n",
    "MAE_advanced = mean_absolute_error(actual_orders, predicted_orders_advanced)\n",
    " \n",
    "# Calculating the Mean Absolute Percentage Error for the advanced model\n",
    "MAPE_advanced = mean_absolute_percentage_error(actual_orders, predicted_orders_advanced)\n",
    " \n",
    "# Displaying the computed metrics\n",
    "print(f\"Advanced Model - Mean Absolute Error (MAE): {MAE_advanced}\")\n",
    "print(f\"Advanced Model - Mean Absolute Percentage Error (MAPE): {MAPE_advanced}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd913dc2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883dae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries:\n",
    "\n",
    "from datetime import timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the advanced LSTM model:\n",
    "\n",
    "def create_advanced_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    # Adding the first LSTM layer with 50 units and ReLU activation.\n",
    "    model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=input_shape))\n",
    "    \n",
    "    # Adding the second LSTM layer with 50 units and ReLU activation.\n",
    "    model.add(LSTM(50, activation='relu'))\n",
    "    \n",
    "    # Adding a dense layer with 25 units and ReLU activation.\n",
    "    model.add(Dense(25, activation='relu'))\n",
    "    \n",
    "    # Adding the output dense layer with a single unit.\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compiling the LSTM model.\n",
    "    model.compile(optimizer='adam', loss='mae')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb7f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the rolling forecast evaluation:\n",
    "\n",
    "def rolling_forecast_evaluation_advanced(df, start_date, forecast_length=30, step=15):\n",
    "    MAEs = []  # List to store Mean Absolute Errors.\n",
    "    MAPEs = []  # List to store Mean Absolute Percentage Errors.\n",
    " \n",
    "    current_date = pd.to_datetime(start_date)\n",
    "    \n",
    "    # Continue as long as the end date doesn't exceed the final date in the dataset.\n",
    "    while current_date < df.index[-1] - timedelta(days=forecast_length):\n",
    "        \n",
    "        # Define training and test periods based on current_date.\n",
    "        train_end = current_date\n",
    "        test_start = train_end + timedelta(days=1)\n",
    "        test_end = test_start + timedelta(days=forecast_length - 1)\n",
    " \n",
    "        # Split data into training and test sets.\n",
    "        train = df[:train_end]\n",
    "        test = df[test_start:test_end]\n",
    " \n",
    "        # Normalize train and test data.\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scaled = scaler.fit_transform(train)\n",
    "        test_scaled = scaler.transform(test)\n",
    " \n",
    "        # Create train and test sets for X (features) and y (target).\n",
    "        X_train, y_train = train_scaled[:, 1:], train_scaled[:, 0]\n",
    "        X_test, y_test = test_scaled[:, 1:], test_scaled[:, 0]\n",
    " \n",
    "        # Adjust data shape for LSTM input.\n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    " \n",
    "        # Create and train the model.\n",
    "        model = create_advanced_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "        model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    " \n",
    "        # Forecast with the trained model.\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Process the predictions for inverse scaling.\n",
    "        inverse_input = np.hstack((y_pred, np.zeros((y_pred.shape[0], df.shape[1]-1))))\n",
    "        predicted = scaler.inverse_transform(inverse_input)[:, 0]\n",
    "        actual = test['orders'].values\n",
    " \n",
    "        # Calculate error metrics.\n",
    "        MAE = mean_absolute_error(actual, predicted)\n",
    "        MAPE = mean_absolute_percentage_error(actual, predicted)\n",
    "        \n",
    "        # Append the errors to their respective lists.\n",
    "        MAEs.append(MAE)\n",
    "        MAPEs.append(MAPE)\n",
    " \n",
    "        # Advance the current_date by the specified step.\n",
    "        current_date += timedelta(days=step)\n",
    " \n",
    "    return MAEs, MAPEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862e539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the function and evaluate the model:\n",
    "\n",
    "# Set the start date for the rolling forecast evaluation.\n",
    "start_date = '2022-01-01'\n",
    "MAEs, MAPEs = rolling_forecast_evaluation_advanced(df, start_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a3f1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the outcome of the cross-validation:\n",
    "\n",
    "# Display the MAE and MAPE for each evaluation window and their averages.\n",
    "print(f\"Average MAE: {np.mean(MAEs)}\")\n",
    "print(f\"Average MAPE: {np.mean(MAPEs)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0eaabd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac0457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the optimizers to be tested\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c762c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the LSTM Model Function based on provided hyperparameters:\n",
    "\n",
    "def create_tuned_model(neurons=50, optimizer=Adam, dropout_rate=0.0):\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add the first LSTM layer with the 'relu' activation function\n",
    "    model.add(LSTM(neurons, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    \n",
    "    # Add the second LSTM layer with the 'relu' activation function\n",
    "    model.add(LSTM(neurons, activation='relu'))\n",
    "    \n",
    "    # Add a Dense layer which has half the number of neurons compared to the LSTM layers and uses the 'relu' activation function\n",
    "    model.add(Dense(neurons//2, activation='relu'))\n",
    "    \n",
    "    # Add an output Dense layer to produce a single output value\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # Compile the model with the provided optimizer and Mean Absolute Error as the loss function\n",
    "    model.compile(optimizer=optimizer(), loss='mae')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50006441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Hyperparameters for Tuning and combine the hyperparameters to test all possible combinations:\n",
    "\n",
    "# Define hyperparameters to be tested\n",
    "neurons = [30, 50]\n",
    "optimizer_classes = [SGD, Adam]\n",
    "dropout_rate = [0.0, 0.2]\n",
    " \n",
    "# Generate all combinations of hyperparameters\n",
    "all_params = [(n, o, d) for n in neurons for o in optimizer_classes for d in dropout_rate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de528b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up variables to track the best hyperparameters and their respective scores:\n",
    "\n",
    "# Initialize variables to track best hyperparameters and scores\n",
    "best_mae_params = None\n",
    "best_mae_score = float('inf')\n",
    "best_mape_params = None\n",
    "best_mape_score = float('inf')\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5dc487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the initial date for the time series and calculate the number of required splits:\n",
    "\n",
    "# Define starting date and calculate number of time series splits\n",
    "start_date = pd.Timestamp('2022-01-01')\n",
    "num_splits = (365 - 30) // 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "374036a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by structuring the loop to go through all hyperparameter combinations:\n",
    "\n",
    "# Loop over all hyperparameter combinations\n",
    "for params in all_params:\n",
    "    total_mae = 0.0\n",
    "    total_mape = 0.0\n",
    "    n, o, d = params\n",
    "    print(n, 0, d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bbfd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation, data preprocessing, model training, prediction, and performance evaluation.\n",
    "\n",
    "# Loop over all hyperparameter combinations\n",
    "for params in all_params:\n",
    "    total_mae = 0.0\n",
    "    total_mape = 0.0\n",
    "    n, o, d = params\n",
    "    \n",
    "    # Sliding window cross-validation over 2022 data\n",
    "    for i in range(0, 365 - 30, 15): \n",
    "        split_date = start_date + pd.Timedelta(days=i)\n",
    "        train = df[df.index < split_date]\n",
    "        test = df[(df.index >= split_date) & (df.index < split_date + pd.Timedelta(days=30))]\n",
    "        \n",
    "        # Preprocessing, scaling\n",
    "        scaler = MinMaxScaler()\n",
    "        train_scaled = scaler.fit_transform(train)\n",
    "        test_scaled = scaler.transform(test)\n",
    "        \n",
    "        X_train, y_train = train_scaled[:, 1:], train_scaled[:, 0]\n",
    "        X_test, y_test = test_scaled[:, 1:], test_scaled[:, 0]\n",
    "        \n",
    "        X_train = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "        X_test = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "        \n",
    "        # Create and fit the model\n",
    "        model = create_tuned_model(neurons=n, optimizer=o, dropout_rate=d)\n",
    "        model.fit(X_train, y_train, epochs=50, verbose=0)\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = model.predict(X_test)\n",
    "        inverse_input = np.hstack((y_pred, np.zeros((y_pred.shape[0], df.shape[1] - 1))))\n",
    "        y_pred_rescaled = scaler.inverse_transform(inverse_input)[:, 0]\n",
    "        \n",
    "        # Convert predictions and test targets back to the original scale\n",
    "        inverse_input_test = np.hstack((y_test.reshape(-1, 1), np.zeros((y_test.shape[0], df.shape[1] - 1))))\n",
    "        y_test_rescaled = scaler.inverse_transform(inverse_input_test)[:, 0]\n",
    "        \n",
    "        # Compute the evaluation metrics.\n",
    "        mae = mean_absolute_error(y_test_rescaled, y_pred_rescaled)\n",
    "        mape = mean_absolute_percentage_error(y_test_rescaled, y_pred_rescaled)\n",
    " \n",
    "        # adding the MAPE and MAE for each fold\n",
    "        total_mae += mae\n",
    "        total_mape += mape\n",
    " \n",
    "    # Computing the Average MAE and MAPE\n",
    "    avg_mae = total_mae / num_splits\n",
    "    avg_mape = total_mape / num_splits\n",
    " \n",
    "    # Storing the results\n",
    "    results.append({'params': params,\n",
    "                    'avg_mae': avg_mae,\n",
    "                    'avg_mape': avg_mape\n",
    "                   })\n",
    " \n",
    "    if avg_mae < best_mae_score:\n",
    "        best_mae_score = avg_mae\n",
    "        best_mae_params = params\n",
    "    \n",
    "    if avg_mape < best_mape_score:\n",
    "        best_mape_score = avg_mape\n",
    "        best_mape_params = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c919066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the best hyperparameters based on evaluation metrics:\n",
    "\n",
    "# Print the best MAE and MAPE scores with their respective hyperparameters\n",
    "print(f\"Best MAE: {best_mae_score}\")\n",
    "print(f\"Best MAPE: {best_mape_score}\")\n",
    "print(f\"Best Hyperparameters for MAE - Neurons: {best_mae_params[0]}, Optimizer: {best_mae_params[1].__name__}, Dropout Rate: {best_mae_params[2]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
